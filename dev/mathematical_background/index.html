<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Mathematical Background · BayesInteractomics.jl</title><meta name="title" content="Mathematical Background · BayesInteractomics.jl"/><meta property="og:title" content="Mathematical Background · BayesInteractomics.jl"/><meta property="twitter:title" content="Mathematical Background · BayesInteractomics.jl"/><meta name="description" content="Documentation for BayesInteractomics.jl."/><meta property="og:description" content="Documentation for BayesInteractomics.jl."/><meta property="twitter:description" content="Documentation for BayesInteractomics.jl."/><meta property="og:url" content="https://ma-seefelder.github.io/BayesInteractomics.jl/mathematical_background/"/><meta property="twitter:url" content="https://ma-seefelder.github.io/BayesInteractomics.jl/mathematical_background/"/><link rel="canonical" href="https://ma-seefelder.github.io/BayesInteractomics.jl/mathematical_background/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">BayesInteractomics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><span class="tocitem">User Guide</span><ul><li><a class="tocitem" href="../data_loading/">Data Loading</a></li><li><a class="tocitem" href="../data_curation/">Data Curation</a></li><li><a class="tocitem" href="../analysis/">Analysis Pipeline</a></li><li><a class="tocitem" href="../model_fitting/">Model Fitting</a></li><li><a class="tocitem" href="../model_evaluation/">Model Evaluation</a></li><li><a class="tocitem" href="../diagnostics/">Diagnostics</a></li><li><a class="tocitem" href="../differential_analysis/">Differential Analysis</a></li><li><a class="tocitem" href="../visualization/">Visualization</a></li><li><a class="tocitem" href="../reports/">Reports</a></li><li><a class="tocitem" href="../network_analysis/">Network Analysis</a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Mathematical Background</a><ul class="internal"><li><a class="tocitem" href="#Overview-of-the-Bayesian-Framework"><span>Overview of the Bayesian Framework</span></a></li><li><a class="tocitem" href="#Model-1:-Beta-Bernoulli-Model-(Detection-Probability)"><span>Model 1: Beta-Bernoulli Model (Detection Probability)</span></a></li><li><a class="tocitem" href="#Model-2:-Hierarchical-Bayesian-Model-(Enrichment)"><span>Model 2: Hierarchical Bayesian Model (Enrichment)</span></a></li><li><a class="tocitem" href="#Model-3:-Bayesian-Linear-Regression-(Dose-Response-Correlation)"><span>Model 3: Bayesian Linear Regression (Dose-Response Correlation)</span></a></li><li><a class="tocitem" href="#Model-Comparison-via-WAIC"><span>Model Comparison via WAIC</span></a></li><li><a class="tocitem" href="#Evidence-Combination"><span>Evidence Combination</span></a></li><li><a class="tocitem" href="#Summary-Statistics"><span>Summary Statistics</span></a></li><li><a class="tocitem" href="#Multiple-Imputation"><span>Multiple Imputation</span></a></li><li><a class="tocitem" href="#Differential-Analysis"><span>Differential Analysis</span></a></li><li><a class="tocitem" href="#Computational-Implementation"><span>Computational Implementation</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Mathematical Background</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Mathematical Background</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ma-seefelder/BayesInteractomics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ma-seefelder/BayesInteractomics.jl/blob/main/docs/src/mathematical_background.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Mathematical-Background"><a class="docs-heading-anchor" href="#Mathematical-Background">Mathematical Background</a><a id="Mathematical-Background-1"></a><a class="docs-heading-anchor-permalink" href="#Mathematical-Background" title="Permalink"></a></h1><p>This document provides a detailed mathematical exposition of the statistical framework implemented in BayesInteractomics. The package integrates three complementary Bayesian models to identify genuine protein-protein interactions from mass spectrometry data.</p><h2 id="Overview-of-the-Bayesian-Framework"><a class="docs-heading-anchor" href="#Overview-of-the-Bayesian-Framework">Overview of the Bayesian Framework</a><a id="Overview-of-the-Bayesian-Framework-1"></a><a class="docs-heading-anchor-permalink" href="#Overview-of-the-Bayesian-Framework" title="Permalink"></a></h2><h3 id="The-Multiple-Evidence-Problem"><a class="docs-heading-anchor" href="#The-Multiple-Evidence-Problem">The Multiple Evidence Problem</a><a id="The-Multiple-Evidence-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#The-Multiple-Evidence-Problem" title="Permalink"></a></h3><p>Identifying true protein interactions requires distinguishing genuine interactors from:</p><ul><li>Non-specific binders (proteins that bind regardless of bait)</li><li>Contaminants (proteins present in controls)</li><li>False positives due to experimental noise</li></ul><p>BayesInteractomics addresses this by evaluating three independent but complementary questions for each candidate protein:</p><ol><li><strong>Detection</strong>: Is the protein consistently detected in samples versus controls?</li><li><strong>Enrichment</strong>: Is the protein quantitatively enriched in samples?</li><li><strong>Correlation</strong>: Does the protein&#39;s abundance correlate with bait levels?</li></ol><p>Each question is answered using a Bayesian model that produces a <strong>Bayes factor</strong> quantifying the evidence for interaction. These Bayes factors are then combined using copula-based mixture models.</p><h3 id="Bayes-Factors"><a class="docs-heading-anchor" href="#Bayes-Factors">Bayes Factors</a><a id="Bayes-Factors-1"></a><a class="docs-heading-anchor-permalink" href="#Bayes-Factors" title="Permalink"></a></h3><p>A Bayes factor compares the evidence for two hypotheses:</p><p class="math-container">\[BF_{10} = \frac{P(D | H_1)}{P(D | H_0)} = \frac{\text{Evidence for } H_1}{\text{Evidence for } H_0}\]</p><p>where <span>$H_1$</span> is the hypothesis of genuine interaction, <span>$H_0$</span> is the null hypothesis (no interaction), and <span>$D$</span> is the observed data.</p><p><strong>Interpretation</strong>:</p><table><tr><th style="text-align: right">Condition</th><th style="text-align: right">Interpretation</th></tr><tr><td style="text-align: right"><span>$BF_{10} &gt; 1$</span></td><td style="text-align: right">Data favor interaction</td></tr><tr><td style="text-align: right"><span>$BF_{10} = 1$</span></td><td style="text-align: right">Data equally support both hypotheses</td></tr><tr><td style="text-align: right"><span>$BF_{10} &lt; 1$</span></td><td style="text-align: right">Data favor null hypothesis</td></tr><tr><td style="text-align: right"><span>$BF_{10} &gt; 10$</span></td><td style="text-align: right">Strong evidence for interaction</td></tr><tr><td style="text-align: right"><span>$BF_{10} &gt; 100$</span></td><td style="text-align: right">Very strong evidence for interaction</td></tr></table><p>Bayes factors provide a continuous measure of evidence that naturally accounts for uncertainty and doesn&#39;t require arbitrary significance thresholds.</p><h2 id="Model-1:-Beta-Bernoulli-Model-(Detection-Probability)"><a class="docs-heading-anchor" href="#Model-1:-Beta-Bernoulli-Model-(Detection-Probability)">Model 1: Beta-Bernoulli Model (Detection Probability)</a><a id="Model-1:-Beta-Bernoulli-Model-(Detection-Probability)-1"></a><a class="docs-heading-anchor-permalink" href="#Model-1:-Beta-Bernoulli-Model-(Detection-Probability)" title="Permalink"></a></h2><h3 id="Biological-Motivation"><a class="docs-heading-anchor" href="#Biological-Motivation">Biological Motivation</a><a id="Biological-Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Biological-Motivation" title="Permalink"></a></h3><p>Genuine interactors should be consistently detected in samples but rarely (or never) in negative controls. The Beta-Bernoulli model evaluates whether the <strong>detection rate</strong> (proportion of replicates where protein is detected) is higher in samples than controls.</p><h3 id="Model-Specification"><a class="docs-heading-anchor" href="#Model-Specification">Model Specification</a><a id="Model-Specification-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Specification" title="Permalink"></a></h3><p>For a protein, let <span>$n_s$</span> be the number of sample replicates, <span>$n_c$</span> the number of control replicates, <span>$k_s$</span> the number of samples where the protein is detected, and <span>$k_c$</span> the number of controls where the protein is detected.</p><p>We model detection as Bernoulli trials:</p><p class="math-container">\[\begin{aligned}
k_s &amp;\sim \text{Binomial}(n_s, \theta_s) \\
k_c &amp;\sim \text{Binomial}(n_c, \theta_c)
\end{aligned}\]</p><p>where <span>$\theta_s$</span> and <span>$\theta_c$</span> are the true detection rates in samples and controls, respectively.</p><h3 id="Prior-Distribution"><a class="docs-heading-anchor" href="#Prior-Distribution">Prior Distribution</a><a id="Prior-Distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Prior-Distribution" title="Permalink"></a></h3><p>We use weakly informative Beta priors for both detection rates:</p><p class="math-container">\[\begin{aligned}
\theta_s &amp;\sim \text{Beta}(3, 3) \\
\theta_c &amp;\sim \text{Beta}(3, 3)
\end{aligned}\]</p><p>The Beta(3,3) prior is centered at 0.5 with moderate uncertainty, expressing weak prior belief that detection rates are neither very low nor very high.</p><h3 id="Posterior-Distribution"><a class="docs-heading-anchor" href="#Posterior-Distribution">Posterior Distribution</a><a id="Posterior-Distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Distribution" title="Permalink"></a></h3><p>Due to conjugacy of the Beta-Binomial model, posteriors are analytical:</p><p class="math-container">\[\begin{aligned}
\theta_s | D &amp;\sim \text{Beta}(3 + k_s, 3 + (n_s - k_s)) \\
\theta_c | D &amp;\sim \text{Beta}(3 + k_c, 3 + (n_c - k_c))
\end{aligned}\]</p><h3 id="Bayes-Factor-Computation"><a class="docs-heading-anchor" href="#Bayes-Factor-Computation">Bayes Factor Computation</a><a id="Bayes-Factor-Computation-1"></a><a class="docs-heading-anchor-permalink" href="#Bayes-Factor-Computation" title="Permalink"></a></h3><p>We test the one-sided hypothesis <span>$H_1\colon \theta_s &gt; \theta_c$</span> (detection rate higher in samples) against <span>$H_0\colon \theta_s \leq \theta_c$</span> (detection rate not higher).</p><p>The posterior probability is estimated via Monte Carlo:</p><p class="math-container">\[p = P(\theta_s &gt; \theta_c | D) = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[\theta_s^{(i)} &gt; \theta_c^{(i)}]\]</p><p>where <span>$\theta_s^{(i)}$</span> and <span>$\theta_c^{(i)}$</span> are samples from the posterior distributions.</p><p>The Bayes factor is computed from posterior and prior odds:</p><p class="math-container">\[BF_{10} = \frac{p / (1-p)}{0.5 / 0.5} = \frac{p}{1-p}\]</p><p>The prior odds are 1:1 (uniform prior on <span>$H_0$</span> vs <span>$H_1$</span>).</p><h3 id="Implementation-Notes"><a class="docs-heading-anchor" href="#Implementation-Notes">Implementation Notes</a><a id="Implementation-Notes-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-Notes" title="Permalink"></a></h3><ul><li>Monte Carlo estimation uses <span>$N = 10^7$</span> samples for high precision</li><li>Detection is defined as non-missing observation in the data matrix</li><li>Missing data are naturally handled (not counted as detection)</li></ul><h2 id="Model-2:-Hierarchical-Bayesian-Model-(Enrichment)"><a class="docs-heading-anchor" href="#Model-2:-Hierarchical-Bayesian-Model-(Enrichment)">Model 2: Hierarchical Bayesian Model (Enrichment)</a><a id="Model-2:-Hierarchical-Bayesian-Model-(Enrichment)-1"></a><a class="docs-heading-anchor-permalink" href="#Model-2:-Hierarchical-Bayesian-Model-(Enrichment)" title="Permalink"></a></h2><h3 id="Biological-Motivation-2"><a class="docs-heading-anchor" href="#Biological-Motivation-2">Biological Motivation</a><a class="docs-heading-anchor-permalink" href="#Biological-Motivation-2" title="Permalink"></a></h3><p>Genuine interactors should show <strong>quantitative enrichment</strong> in samples compared to controls. The Hierarchical Bayesian Model (HBM) estimates the log2 fold change (log2FC) while accounting for:</p><ul><li>Protocol-level heterogeneity (different experimental methods)</li><li>Experiment-level batch effects</li><li>Missing data across replicates</li></ul><h3 id="Model-Specification-2"><a class="docs-heading-anchor" href="#Model-Specification-2">Model Specification</a><a class="docs-heading-anchor-permalink" href="#Model-Specification-2" title="Permalink"></a></h3><p>Let <span>$y_{pej}$</span> denote the log-transformed intensity for protocol <span>$p \in \{1, \ldots, P\}$</span>, experiment <span>$e \in \{1, \ldots, E_p\}$</span> within protocol <span>$p$</span>, and sample <span>$j$</span> (either control or bait).</p><h4 id="Likelihood"><a class="docs-heading-anchor" href="#Likelihood">Likelihood</a><a id="Likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Likelihood" title="Permalink"></a></h4><p class="math-container">\[y_{pej} | \mu_{pe}, \sigma^2_{pe} \sim \mathcal{N}(\mu_{pe}, \sigma^2_{pe})\]</p><p>where <span>$\mu_{pe}$</span> is the mean intensity and <span>$\sigma^2_{pe}$</span> is the variance for experiment <span>$e$</span> in protocol <span>$p$</span>.</p><h4 id="Hierarchical-Structure"><a class="docs-heading-anchor" href="#Hierarchical-Structure">Hierarchical Structure</a><a id="Hierarchical-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#Hierarchical-Structure" title="Permalink"></a></h4><p><strong>Protocol-level parameters</strong> (shared across experiments within a protocol):</p><p class="math-container">\[\begin{aligned}
\mu_{pe}^{\text{control}} | \mu_p^0, \tau_p^2 &amp;\sim \mathcal{N}(\mu_p^0, \tau_p^2) \\
\mu_{pe}^{\text{sample}} | \mu_p^0, \log_2 FC_p, \tau_p^2 &amp;\sim \mathcal{N}(\mu_p^0 + \log_2 FC_p, \tau_p^2)
\end{aligned}\]</p><p>where <span>$\mu_p^0$</span> is the baseline intensity for protocol <span>$p$</span>, <span>$\log_2 FC_p$</span> is the log2 fold change for protocol <span>$p$</span> (parameter of interest), and <span>$\tau_p^2$</span> is the between-experiment variance within protocol <span>$p$</span>.</p><p><strong>Experiment-level variance</strong>:</p><p class="math-container">\[\sigma^2_{pe} \sim \text{InverseGamma}(\alpha_{\sigma}, \beta_{\sigma})\]</p><h4 id="Priors"><a class="docs-heading-anchor" href="#Priors">Priors</a><a id="Priors-1"></a><a class="docs-heading-anchor-permalink" href="#Priors" title="Permalink"></a></h4><p><strong>Log2 fold change</strong> (weakly informative):</p><p class="math-container">\[\log_2 FC_p \sim \mathcal{N}(0, 10)\]</p><p><strong>Baseline intensity</strong>:</p><p class="math-container">\[\mu_p^0 = \frac{1}{E_p} \sum_{e=1}^{E_p} \bar{y}_{pe}^{\text{control}}\]</p><p>where <span>$\bar{y}_{pe}^{\text{control}}$</span> is the empirical mean of control samples.</p><p><strong>Between-experiment variance</strong>:</p><p class="math-container">\[\tau_p^2 = \max\left(\sigma_p^2 - \bar{\sigma}^2_{pe}, \epsilon\right)\]</p><p>where <span>$\sigma_p^2$</span> is the empirical variance of experiment means, <span>$\bar{\sigma}^2_{pe}$</span> is the average within-experiment variance, and <span>$\epsilon = 10^{-6}$</span> prevents numerical issues.</p><p><strong>Within-experiment variance</strong> (conjugate prior):</p><p class="math-container">\[\alpha_{\sigma} = 2, \quad \beta_{\sigma} = 0.5\]</p><h3 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h3><p>Posterior inference is performed using <strong>variational Bayes</strong> via RxInfer.jl, which approximates the posterior through optimization rather than sampling. This provides:</p><ul><li>Fast convergence (seconds per protein)</li><li>Automatic convergence diagnostics</li><li>Full posterior distributions for all parameters</li></ul><h3 id="Bayes-Factor-Computation-2"><a class="docs-heading-anchor" href="#Bayes-Factor-Computation-2">Bayes Factor Computation</a><a class="docs-heading-anchor-permalink" href="#Bayes-Factor-Computation-2" title="Permalink"></a></h3><p>The Bayes factor for enrichment tests <span>$H_1\colon \log_2 FC_p &gt; 0$</span> (enrichment in samples) against <span>$H_0\colon \log_2 FC_p \leq 0$</span> (no enrichment).</p><p>From the posterior distribution <span>$q(\log_2 FC_p | D)$</span>:</p><p class="math-container">\[BF_{10} = \frac{P(\log_2 FC_p &gt; 0 | D)}{P(\log_2 FC_p \leq 0 | D)} = \frac{p}{1-p}\]</p><p>where <span>$p = \int_0^{\infty} q(\log_2 FC_p | D) \, d(\log_2 FC_p)$</span>.</p><h3 id="Multiple-Protocols"><a class="docs-heading-anchor" href="#Multiple-Protocols">Multiple Protocols</a><a id="Multiple-Protocols-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Protocols" title="Permalink"></a></h3><p>For datasets with multiple protocols, we obtain protocol-specific Bayes factors <span>$BF_1, BF_2, \ldots, BF_P$</span>. The overall enrichment Bayes factor is:</p><p class="math-container">\[BF_{\text{enrichment}} = \prod_{p=1}^{P} BF_p\]</p><p>This assumes conditional independence of protocols given the hypothesis.</p><h2 id="Model-3:-Bayesian-Linear-Regression-(Dose-Response-Correlation)"><a class="docs-heading-anchor" href="#Model-3:-Bayesian-Linear-Regression-(Dose-Response-Correlation)">Model 3: Bayesian Linear Regression (Dose-Response Correlation)</a><a id="Model-3:-Bayesian-Linear-Regression-(Dose-Response-Correlation)-1"></a><a class="docs-heading-anchor-permalink" href="#Model-3:-Bayesian-Linear-Regression-(Dose-Response-Correlation)" title="Permalink"></a></h2><h3 id="Biological-Motivation-3"><a class="docs-heading-anchor" href="#Biological-Motivation-3">Biological Motivation</a><a class="docs-heading-anchor-permalink" href="#Biological-Motivation-3" title="Permalink"></a></h3><p>Genuine interactors often show a <strong>dose-response relationship</strong>: their abundance correlates with the bait protein&#39;s abundance. If bait expression varies across samples (e.g., due to transfection efficiency), true interactors should track these variations, while contaminants should not.</p><h3 id="Model-Specification-(Normal-Likelihood)"><a class="docs-heading-anchor" href="#Model-Specification-(Normal-Likelihood)">Model Specification (Normal Likelihood)</a><a id="Model-Specification-(Normal-Likelihood)-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Specification-(Normal-Likelihood)" title="Permalink"></a></h3><p>Let <span>$y_i$</span> be the candidate protein intensity in sample <span>$i$</span>, <span>$x_i$</span> the bait protein (reference) intensity in sample <span>$i$</span>, and <span>$i \in \{1, \ldots, N\}$</span> the sample indices.</p><h4 id="Likelihood-2"><a class="docs-heading-anchor" href="#Likelihood-2">Likelihood</a><a class="docs-heading-anchor-permalink" href="#Likelihood-2" title="Permalink"></a></h4><p class="math-container">\[y_i | \beta_0, \beta_1, \sigma^2 \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2)\]</p><p>where <span>$\beta_0$</span> is the intercept, <span>$\beta_1$</span> is the slope (correlation strength — parameter of interest), and <span>$\sigma^2$</span> is the residual variance.</p><h4 id="Priors-2"><a class="docs-heading-anchor" href="#Priors-2">Priors</a><a class="docs-heading-anchor-permalink" href="#Priors-2" title="Permalink"></a></h4><p><strong>Slope</strong> (weakly informative):</p><p class="math-container">\[\beta_1 \sim \mathcal{N}(0, 10)\]</p><p><strong>Intercept</strong> (weakly informative):</p><p class="math-container">\[\beta_0 \sim \mathcal{N}(0, 100)\]</p><p><strong>Residual variance</strong> (conjugate):</p><p class="math-container">\[\sigma^2 \sim \text{InverseGamma}(2, 0.5)\]</p><h3 id="Hierarchical-Extension-for-Multiple-Protocols"><a class="docs-heading-anchor" href="#Hierarchical-Extension-for-Multiple-Protocols">Hierarchical Extension for Multiple Protocols</a><a id="Hierarchical-Extension-for-Multiple-Protocols-1"></a><a class="docs-heading-anchor-permalink" href="#Hierarchical-Extension-for-Multiple-Protocols" title="Permalink"></a></h3><p>When multiple protocols are present, we use protocol-specific slopes <span>$\beta_{1p}$</span> with a hierarchical structure:</p><p class="math-container">\[\beta_{1p} | \mu_{\beta}, \tau_{\beta}^2 \sim \mathcal{N}(\mu_{\beta}, \tau_{\beta}^2)\]</p><p>where <span>$\mu_{\beta}$</span> is the population mean slope (overall correlation) and <span>$\tau_{\beta}^2$</span> is the between-protocol variance in slopes.</p><p><strong>Hyperpriors</strong>:</p><p class="math-container">\[\begin{aligned}
\mu_{\beta} &amp;\sim \mathcal{N}(0, 10) \\
\tau_{\beta}^2 &amp;\sim \text{Gamma}(1, 1)
\end{aligned}\]</p><h3 id="Inference-2"><a class="docs-heading-anchor" href="#Inference-2">Inference</a><a class="docs-heading-anchor-permalink" href="#Inference-2" title="Permalink"></a></h3><p>Posterior inference uses variational Bayes via RxInfer.jl, yielding posterior distributions for all parameters.</p><h3 id="Bayes-Factor-Computation-3"><a class="docs-heading-anchor" href="#Bayes-Factor-Computation-3">Bayes Factor Computation</a><a class="docs-heading-anchor-permalink" href="#Bayes-Factor-Computation-3" title="Permalink"></a></h3><p>The Bayes factor tests <span>$H_1\colon \beta_1 &gt; 0$</span> (positive correlation with bait) against <span>$H_0\colon \beta_1 \leq 0$</span> (no positive correlation).</p><p class="math-container">\[BF_{10} = \frac{P(\beta_1 &gt; 0 | D)}{P(\beta_1 \leq 0 | D)} = \frac{p}{1-p}\]</p><p>where <span>$p$</span> is computed from the posterior distribution.</p><p>For multiple protocols:</p><p class="math-container">\[BF_{\text{correlation}} = \prod_{p=1}^{P} BF_p\]</p><h3 id="Robust-Extension:-Student-t-via-Scale-Mixture"><a class="docs-heading-anchor" href="#Robust-Extension:-Student-t-via-Scale-Mixture">Robust Extension: Student-t via Scale Mixture</a><a id="Robust-Extension:-Student-t-via-Scale-Mixture-1"></a><a class="docs-heading-anchor-permalink" href="#Robust-Extension:-Student-t-via-Scale-Mixture" title="Permalink"></a></h3><p>The standard Normal likelihood assumes homogeneous residual variance, making the regression sensitive to outliers. Proteomics data frequently contain aberrant intensity values caused by misidentifications, interference, or carry-over. A <strong>robust regression</strong> replaces the Normal likelihood with a heavier-tailed Student-t distribution, implemented through a Normal–Gamma scale mixture that is fully compatible with variational message passing.</p><h4 id="Scale-Mixture-Representation"><a class="docs-heading-anchor" href="#Scale-Mixture-Representation">Scale-Mixture Representation</a><a id="Scale-Mixture-Representation-1"></a><a class="docs-heading-anchor-permalink" href="#Scale-Mixture-Representation" title="Permalink"></a></h4><p>Each observation receives its own precision <span>$\tau_i$</span>, drawn from a Gamma distribution:</p><p class="math-container">\[\begin{aligned}
\tau_i &amp;\sim \text{Gamma}\!\left(\frac{\nu}{2},\; \text{scale} = \frac{\tau_{\text{base}}}{\nu/2}\right), \quad \mathbb{E}[\tau_i] = \tau_{\text{base}} \\
y_i \mid \mu_i, \tau_i &amp;\sim \mathcal{N}(\mu_i,\; \text{precision} = \tau_i)
\end{aligned}\]</p><p>Marginalizing over <span>$\tau_i$</span> recovers a Student-t distribution:</p><p class="math-container">\[y_i \mid \mu_i \;\sim\; \text{Student-}t\!\left(\nu,\; \mu_i,\; \tau_{\text{base}}\right)\]</p><p>The degrees-of-freedom parameter <span>$\nu$</span> controls tail heaviness: smaller <span>$\nu$</span> yields heavier tails and greater outlier robustness; as <span>$\nu \to \infty$</span> the model reduces to Normal regression.</p><h4 id="Hierarchical-Prior-Structure"><a class="docs-heading-anchor" href="#Hierarchical-Prior-Structure">Hierarchical Prior Structure</a><a id="Hierarchical-Prior-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#Hierarchical-Prior-Structure" title="Permalink"></a></h4><p>The prior structure is identical to the Normal model. For the multi-protocol case:</p><table><tr><th style="text-align: right">Parameter</th><th style="text-align: right">Prior</th></tr><tr><td style="text-align: right"><span>$\mu_\alpha$</span> (hyper-mean intercept)</td><td style="text-align: right"><span>$\mathcal{N}(0,\; (0.3/1.96)^2)$</span></td></tr><tr><td style="text-align: right"><span>$\mu_\beta$</span> (hyper-mean slope)</td><td style="text-align: right"><span>$\mathcal{N}(\hat{\mu}_0,\; \sigma_0^2)$</span></td></tr><tr><td style="text-align: right"><span>$\sigma_\alpha$</span> (hyper-precision intercept)</td><td style="text-align: right"><span>$\text{Gamma}(6.304,\; \text{scale}=7.932)$</span></td></tr><tr><td style="text-align: right"><span>$\sigma_\beta$</span> (hyper-precision slope)</td><td style="text-align: right"><span>$\text{Gamma}(10,\; \text{scale}=0.3)$</span></td></tr><tr><td style="text-align: right"><span>$\alpha_k$</span> (per-protocol intercept)</td><td style="text-align: right"><span>$\mathcal{N}(\mu_\alpha,\; \text{precision}=\sigma_\alpha)$</span></td></tr><tr><td style="text-align: right"><span>$\beta_k$</span> (per-protocol slope)</td><td style="text-align: right"><span>$\mathcal{N}(\mu_\beta,\; \text{precision}=\sigma_\beta)$</span></td></tr></table><p>The empirical Bayes hyperparameters <span>$\hat{\mu}_0$</span> and <span>$\sigma_0^2$</span> are estimated from OLS on the pooled data, identical to the Normal model.</p><h4 id="Empirical-Bayes-for-\\tau_{\\text{base}}"><a class="docs-heading-anchor" href="#Empirical-Bayes-for-\\tau_{\\text{base}}">Empirical Bayes for <span>$\tau_{\text{base}}$</span></a><a id="Empirical-Bayes-for-\\tau_{\\text{base}}-1"></a><a class="docs-heading-anchor-permalink" href="#Empirical-Bayes-for-\\tau_{\\text{base}}" title="Permalink"></a></h4><p>The baseline precision is set to the inverse residual variance from an OLS fit:</p><p class="math-container">\[\tau_{\text{base}} = \frac{1}{\text{Var}(\hat{\varepsilon}_{\text{OLS}})}\]</p><p>This anchors the Student-t scale to the data, ensuring that <span>$\mathbb{E}[\tau_i] = \tau_{\text{base}}$</span> matches the observed noise level.</p><h4 id="Default-Configuration"><a class="docs-heading-anchor" href="#Default-Configuration">Default Configuration</a><a id="Default-Configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Default-Configuration" title="Permalink"></a></h4><p>The degrees-of-freedom parameter defaults to <span>$\nu = 5$</span>, which provides moderately heavy tails. When model comparison is enabled (see next section), <span>$\nu$</span> is optimized over <span>$[3, 50]$</span> by minimizing WAIC.</p><h2 id="Model-Comparison-via-WAIC"><a class="docs-heading-anchor" href="#Model-Comparison-via-WAIC">Model Comparison via WAIC</a><a id="Model-Comparison-via-WAIC-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Comparison-via-WAIC" title="Permalink"></a></h2><p>The Widely Applicable Information Criterion (WAIC; Watanabe, 2010) provides a principled method for comparing the Normal and robust (Student-t) regression models. Unlike AIC or BIC, WAIC is fully Bayesian and uses the entire posterior distribution rather than a point estimate.</p><h3 id="WAIC-Definition"><a class="docs-heading-anchor" href="#WAIC-Definition">WAIC Definition</a><a id="WAIC-Definition-1"></a><a class="docs-heading-anchor-permalink" href="#WAIC-Definition" title="Permalink"></a></h3><p>Given <span>$S$</span> posterior draws <span>$\theta^{(1)}, \ldots, \theta^{(S)}$</span> and <span>$n$</span> observations, WAIC is defined as:</p><p class="math-container">\[\text{WAIC} = -2\left(\text{lppd} - p_{\text{waic}}\right)\]</p><p>where the <strong>log pointwise predictive density</strong> and the <strong>effective number of parameters</strong> are:</p><p class="math-container">\[\begin{aligned}
\text{lppd} &amp;= \sum_{i=1}^{n} \log\!\left(\frac{1}{S} \sum_{s=1}^{S} p(y_i \mid \theta^{(s)})\right) \\
p_{\text{waic}} &amp;= \sum_{i=1}^{n} \text{Var}_{s}\!\left(\log p(y_i \mid \theta^{(s)})\right)
\end{aligned}\]</p><p>Lower WAIC indicates better out-of-sample predictive performance. The implementation uses <span>$S = 1000$</span> posterior draws from the VMP approximate posteriors.</p><h3 id="Normal-vs-Robust-Regression-Comparison"><a class="docs-heading-anchor" href="#Normal-vs-Robust-Regression-Comparison">Normal vs Robust Regression Comparison</a><a id="Normal-vs-Robust-Regression-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Normal-vs-Robust-Regression-Comparison" title="Permalink"></a></h3><p>To compare the two regression models, we compute WAIC for each and take the difference:</p><p class="math-container">\[\Delta\text{WAIC} = \text{WAIC}_{\text{normal}} - \text{WAIC}_{\text{robust}}\]</p><p>A positive <span>$\Delta\text{WAIC}$</span> favors the robust model (lower WAIC). The standard error of the difference is estimated from pointwise WAIC differences:</p><p class="math-container">\[\text{SE}_\Delta = \sqrt{n \cdot \text{Var}(w_i^{\text{normal}} - w_i^{\text{robust}})}\]</p><p>where <span>$w_i$</span> denotes the pointwise WAIC contribution of observation <span>$i$</span>. When <span>$|\Delta\text{WAIC}| &gt; 2 \cdot \text{SE}_\Delta$</span>, the difference is considered meaningful (Vehtari et al., 2017).</p><h3 id="Degrees-of-Freedom-Optimization"><a class="docs-heading-anchor" href="#Degrees-of-Freedom-Optimization">Degrees-of-Freedom Optimization</a><a id="Degrees-of-Freedom-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Degrees-of-Freedom-Optimization" title="Permalink"></a></h3><p>When the robust model is selected, the degrees-of-freedom parameter <span>$\nu$</span> can be optimized to minimize WAIC. BayesInteractomics uses <strong>Brent&#39;s method</strong> to search over <span>$\nu \in [3, 50]$</span> with a tolerance of 0.5 (finer precision is not meaningful given WAIC uncertainty). A fixed random seed ensures a deterministic, smooth objective surface for the optimizer.</p><p>The optimization procedure:</p><ol><li>Compute WAIC for the Normal model once (baseline).</li><li>For each candidate <span>$\nu$</span>, fit the robust model across all proteins and compute WAIC.</li><li>Return the <span>$\nu$</span> that minimizes WAIC, along with the <span>$\Delta\text{WAIC}$</span> relative to the Normal baseline.</li></ol><h2 id="Evidence-Combination"><a class="docs-heading-anchor" href="#Evidence-Combination">Evidence Combination</a><a id="Evidence-Combination-1"></a><a class="docs-heading-anchor-permalink" href="#Evidence-Combination" title="Permalink"></a></h2><h3 id="The-Combination-Problem"><a class="docs-heading-anchor" href="#The-Combination-Problem">The Combination Problem</a><a id="The-Combination-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#The-Combination-Problem" title="Permalink"></a></h3><p>We now have three Bayes factors for each protein: <span>$BF_{\text{detection}}$</span> (detection evidence), <span>$BF_{\text{enrichment}}$</span> (enrichment evidence), and <span>$BF_{\text{correlation}}$</span> (correlation evidence).</p><p>These are <strong>not independent</strong>: for example, enriched proteins are more likely to be consistently detected. Simple multiplication (independence assumption) would be incorrect.</p><p><strong>Solution</strong>: Model the joint distribution of Bayes factors using <strong>copulas</strong>, which flexibly capture dependencies while allowing arbitrary marginals. BayesInteractomics also provides a <strong>latent class model</strong> and <strong>Bayesian model averaging</strong> across both combination methods.</p><h3 id="Copula-Based-Combination"><a class="docs-heading-anchor" href="#Copula-Based-Combination">Copula-Based Combination</a><a id="Copula-Based-Combination-1"></a><a class="docs-heading-anchor-permalink" href="#Copula-Based-Combination" title="Permalink"></a></h3><h4 id="Copula-Theory"><a class="docs-heading-anchor" href="#Copula-Theory">Copula Theory</a><a id="Copula-Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Copula-Theory" title="Permalink"></a></h4><p>A copula <span>$C$</span> is a multivariate distribution on <span>$[0,1]^d$</span> with uniform marginals. By Sklar&#39;s theorem, any multivariate distribution <span>$F$</span> can be decomposed as:</p><p class="math-container">\[F(x_1, \ldots, x_d) = C(F_1(x_1), \ldots, F_d(x_d))\]</p><p>where <span>$F_i$</span> are the marginal distributions and <span>$C$</span> is the copula capturing dependence.</p><h4 id="Mixture-Copula-Model"><a class="docs-heading-anchor" href="#Mixture-Copula-Model">Mixture Copula Model</a><a id="Mixture-Copula-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Mixture-Copula-Model" title="Permalink"></a></h4><p>The distribution of Bayes factors arises from a mixture of two populations:</p><ul><li><strong><span>$H_0$</span> population</strong>: Non-interacting proteins (null hypothesis true)</li><li><strong><span>$H_1$</span> population</strong>: Genuine interactors (alternative hypothesis true)</li></ul><p>Let <span>$\mathbf{BF} = (BF_{\text{detection}}, BF_{\text{enrichment}}, BF_{\text{correlation}})$</span> be the triplet of Bayes factors.</p><p><strong>Mixture model</strong>:</p><p class="math-container">\[F(\mathbf{BF}) = \pi_0 \cdot F_{H_0}(\mathbf{BF}) + \pi_1 \cdot F_{H_1}(\mathbf{BF})\]</p><p>where <span>$\pi_0$</span> is the proportion of non-interactors, <span>$\pi_1 = 1 - \pi_0$</span> is the proportion of true interactors, <span>$F_{H_0}$</span> is the joint distribution under <span>$H_0$</span> (modeled by copula <span>$C_0$</span>), and <span>$F_{H_1}$</span> is the joint distribution under <span>$H_1$</span> (modeled by copula <span>$C_1$</span>).</p><p><strong>Copula specification</strong> — for each component <span>$k \in \{0, 1\}$</span>:</p><p class="math-container">\[F_{H_k}(\mathbf{BF}) = C_k\left(G_1(BF_{\text{detection}}), G_2(BF_{\text{enrichment}}), G_3(BF_{\text{correlation}})\right)\]</p><p>where <span>$C_k$</span> is the copula for component <span>$k$</span> (e.g., Clayton, Gumbel, Frank, Gaussian) and <span>$G_i$</span> is the marginal cumulative distribution for evidence type <span>$i$</span>.</p><p>BayesInteractomics supports multiple copula families:</p><ul><li><strong>Clayton</strong>: Models lower tail dependence (joint low values)</li><li><strong>Gumbel</strong>: Models upper tail dependence (joint high values)</li><li><strong>Frank</strong>: Symmetric dependence</li><li><strong>Gaussian</strong>: Linear correlation structure</li><li><strong>Joe</strong>: Asymmetric upper tail dependence</li></ul><h4 id="EM-Algorithm"><a class="docs-heading-anchor" href="#EM-Algorithm">EM Algorithm</a><a id="EM-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#EM-Algorithm" title="Permalink"></a></h4><p>The mixture model parameters <span>$\Theta = \{\pi_0, \pi_1, C_0, C_1, G_1, G_2, G_3\}$</span> are estimated using the EM algorithm.</p><p><strong>E-Step</strong> — compute posterior probability that protein <span>$i$</span> belongs to <span>$H_1$</span>:</p><p class="math-container">\[\gamma_i^{(t)} = \frac{\pi_1^{(t)} \cdot f_{H_1}(\mathbf{BF}_i | \Theta^{(t)})}{\pi_0^{(t)} \cdot f_{H_0}(\mathbf{BF}_i | \Theta^{(t)}) + \pi_1^{(t)} \cdot f_{H_1}(\mathbf{BF}_i | \Theta^{(t)})}\]</p><p>where <span>$f_{H_k}$</span> is the density corresponding to <span>$F_{H_k}$</span>.</p><p><strong>M-Step</strong> — update parameters to maximize expected complete-data log-likelihood:</p><p>Mixture weights:</p><p class="math-container">\[\pi_1^{(t+1)} = \frac{1}{N} \sum_{i=1}^{N} \gamma_i^{(t)}\]</p><p>Copula parameters: fit <span>$C_0$</span> and <span>$C_1$</span> using weighted data — <span>$C_0$</span> is fit to proteins with weights <span>$(1 - \gamma_i^{(t)})$</span> and <span>$C_1$</span> is fit to proteins with weights <span>$\gamma_i^{(t)}$</span>.</p><p>Marginals: fit <span>$G_1, G_2, G_3$</span> using kernel density estimation or empirical CDFs.</p><p><strong>Initialization</strong>:</p><ul><li>The <strong><span>$H_0$</span> component</strong> is initialized using proteins with all Bayes factors &lt; 1 (strong evidence against interaction)</li><li>The <strong><span>$H_1$</span> component</strong> is initialized using proteins with all Bayes factors &gt; threshold (e.g., &gt; 3)</li><li>The <strong>mixture weight</strong> is set to <span>$\pi_1^{(0)} = 0.1$</span> (conservative initial estimate)</li></ul><p><strong>Convergence</strong> — iterate E-step and M-step until:</p><p class="math-container">\[\frac{|\pi_1^{(t+1)} - \pi_1^{(t)}|}{|\pi_1^{(t)}|} &lt; \epsilon\]</p><p>Typically <span>$\epsilon = 10^{-4}$</span> and convergence occurs in 10–50 iterations.</p><h4 id="Combined-Bayes-Factor"><a class="docs-heading-anchor" href="#Combined-Bayes-Factor">Combined Bayes Factor</a><a id="Combined-Bayes-Factor-1"></a><a class="docs-heading-anchor-permalink" href="#Combined-Bayes-Factor" title="Permalink"></a></h4><p>After EM convergence, the combined Bayes factor for protein <span>$i$</span> is:</p><p class="math-container">\[BF_{\text{combined},i} = \frac{f_{H_1}(\mathbf{BF}_i)}{f_{H_0}(\mathbf{BF}_i)}\]</p><p>This is the likelihood ratio using the fitted copula densities.</p><h4 id="Posterior-Probability"><a class="docs-heading-anchor" href="#Posterior-Probability">Posterior Probability</a><a id="Posterior-Probability-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Probability" title="Permalink"></a></h4><p>Assuming uniform prior <span>$P(H_1) = 0.5$</span>, the posterior probability of interaction is:</p><p class="math-container">\[P(H_1 | \mathbf{BF}_i) = \frac{BF_{\text{combined},i}}{1 + BF_{\text{combined},i}}\]</p><p>Alternatively, using the EM-estimated mixture proportion:</p><p class="math-container">\[P(H_1 | \mathbf{BF}_i) = \gamma_i\]</p><h3 id="Latent-Class-Model"><a class="docs-heading-anchor" href="#Latent-Class-Model">Latent Class Model</a><a id="Latent-Class-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Latent-Class-Model" title="Permalink"></a></h3><p>The latent class model is a simpler alternative to copula-based combination that models the joint distribution of log-Bayes factors directly as a Gaussian mixture. It assumes <strong>conditional independence</strong> of the three evidence arms given the latent interaction status, which trades flexibility in dependence modeling for computational simplicity and robustness.</p><h4 id="Model-Specification-3"><a class="docs-heading-anchor" href="#Model-Specification-3">Model Specification</a><a class="docs-heading-anchor-permalink" href="#Model-Specification-3" title="Permalink"></a></h4><p>Each protein has a latent class <span>$z_i \in \{0, 1\}$</span> indicating Background or Interaction status. The three-dimensional score vector <span>$\mathbf{s}_i = (\log BF_{\text{enrich},i},\; \log BF_{\text{corr},i},\; \log BF_{\text{detect},i})$</span> is modeled as:</p><p class="math-container">\[\begin{aligned}
z_i &amp;\sim \text{Categorical}(\pi_0, \pi_1), \quad \pi_0 + \pi_1 = 1 \\
p(\mathbf{s}_i \mid z_i = k) &amp;= \prod_{d=1}^{3} \mathcal{N}(s_{id} \mid \mu_{dk},\; \sigma^2_{dk})
\end{aligned}\]</p><p>The conditional independence assumption means that each evidence dimension contributes independently to the class assignment, given the latent state. This is a 2-component, 3-dimensional Gaussian mixture with diagonal covariance.</p><h4 id="Data-Preprocessing"><a class="docs-heading-anchor" href="#Data-Preprocessing">Data Preprocessing</a><a id="Data-Preprocessing-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Preprocessing" title="Permalink"></a></h4><p>Before EM fitting, Bayes factors are transformed and optionally winsorized:</p><ol><li><strong>Log-transform</strong>: <span>$s_{id} = \log(BF_{id})$</span>. Log-Bayes factors are approximately Normal by the CLT, with <span>$\log(BF) = 0$</span> representing no evidence.</li><li><strong>Winsorization</strong> (optional, default on): extreme values are clamped to the 1st and 99th percentiles. This protects the EM parameter estimates from extreme outliers. The original (non-winsorized) log-BFs are retained for posterior computation.</li></ol><h4 id="EM-Algorithm-2"><a class="docs-heading-anchor" href="#EM-Algorithm-2">EM Algorithm</a><a class="docs-heading-anchor-permalink" href="#EM-Algorithm-2" title="Permalink"></a></h4><p>The EM algorithm iterates between responsibility computation and parameter updates:</p><p><strong>E-Step</strong> — for each protein <span>$i$</span>, compute the posterior probability of belonging to the interaction class:</p><p class="math-container">\[\gamma_i = \frac{\pi_1 \prod_{d} \mathcal{N}(s_{id} \mid \mu_{d1}, \sigma^2_{d1})}{\pi_0 \prod_{d} \mathcal{N}(s_{id} \mid \mu_{d0}, \sigma^2_{d0}) + \pi_1 \prod_{d} \mathcal{N}(s_{id} \mid \mu_{d1}, \sigma^2_{d1})}\]</p><p><strong>M-Step</strong> — update parameters using the responsibilities:</p><p class="math-container">\[\begin{aligned}
\pi_k^{\text{new}} &amp;= \frac{N_k + \alpha_k - 1}{N + \sum_j \alpha_j - 2}, \quad N_k = \sum_i \gamma_{ik} \\
\mu_{dk}^{\text{new}} &amp;= \frac{\sum_i \gamma_{ik} \, s_{id}}{N_k} \\
\sigma_{dk}^{\text{new}} &amp;= \max\!\left(\sqrt{\frac{\sum_i \gamma_{ik} (s_{id} - \mu_{dk})^2}{N_k}},\; \sigma_{\text{floor}}\right)
\end{aligned}\]</p><p>The mixing weights receive a Dirichlet prior with <span>$\boldsymbol{\alpha} = (10, 1)$</span>, encoding a prior expectation that most proteins are non-interactors.</p><p><strong>Label ordering constraint</strong>: after each M-step, if <span>$\mu_{\text{enrich},1} &lt; \mu_{\text{enrich},0}$</span>, all parameters and responsibilities between the two components are swapped. This ensures the interaction class always has a higher mean enrichment score.</p><p><strong>Convergence</strong>: the algorithm terminates when the relative change in log-likelihood falls below <span>$10^{-6}$</span>, or after a maximum of 100 iterations.</p><h4 id="Posterior-Computation-and-Monotonicity-Correction"><a class="docs-heading-anchor" href="#Posterior-Computation-and-Monotonicity-Correction">Posterior Computation and Monotonicity Correction</a><a id="Posterior-Computation-and-Monotonicity-Correction-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Computation-and-Monotonicity-Correction" title="Permalink"></a></h4><p>Final posteriors are computed on the <strong>original (non-winsorized)</strong> log-BF values using the EM-fitted parameters. A monotonicity correction prevents proteins with extremely strong evidence from being penalized:</p><p>For each dimension <span>$d$</span>, if <span>$s_{id} &gt; \mu_{d1}$</span> (the protein&#39;s score exceeds the interaction-class mean), the per-dimension log-likelihood ratio is floored at zero:</p><p class="math-container">\[\text{LLR}_d = \begin{cases}
\log \frac{\mathcal{N}(s_{id} \mid \mu_{d1}, \sigma^2_{d1})}{\mathcal{N}(s_{id} \mid \mu_{d0}, \sigma^2_{d0})} &amp; \text{if } s_{id} \leq \mu_{d1} \\[6pt]
\max\!\left(\log \frac{\mathcal{N}(s_{id} \mid \mu_{d1}, \sigma^2_{d1})}{\mathcal{N}(s_{id} \mid \mu_{d0}, \sigma^2_{d0})},\; 0\right) &amp; \text{if } s_{id} &gt; \mu_{d1}
\end{cases}\]</p><p>The posterior probability is then:</p><p class="math-container">\[P(z_i = 1 \mid \mathbf{s}_i) = \frac{1}{1 + \exp\!\left(-\log\frac{\pi_1}{\pi_0} - \sum_d \text{LLR}_d\right)}\]</p><h3 id="Bayesian-Model-Averaging-(BMA)"><a class="docs-heading-anchor" href="#Bayesian-Model-Averaging-(BMA)">Bayesian Model Averaging (BMA)</a><a id="Bayesian-Model-Averaging-(BMA)-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Model-Averaging-(BMA)" title="Permalink"></a></h3><p>When both copula and latent class combination methods are available, Bayesian Model Averaging (BMA) provides a principled way to combine their posterior probabilities, weighting each method by how well it fits the data.</p><h4 id="BIC-Computation-and-Model-Weights"><a class="docs-heading-anchor" href="#BIC-Computation-and-Model-Weights">BIC Computation and Model Weights</a><a id="BIC-Computation-and-Model-Weights-1"></a><a class="docs-heading-anchor-permalink" href="#BIC-Computation-and-Model-Weights" title="Permalink"></a></h4><p>The Bayesian Information Criterion approximates the log marginal likelihood for each combination model <span>$m$</span>:</p><p class="math-container">\[\text{BIC}_m = -2 \log \hat{L}_m + k_m \log n\]</p><p>where <span>$\hat{L}_m$</span> is the maximized likelihood, <span>$k_m$</span> is the number of parameters, and <span>$n$</span> is the number of proteins.</p><p>The parameter counts are:</p><table><tr><th style="text-align: right">Model</th><th style="text-align: right">Parameters</th><th style="text-align: right">Total <span>$k$</span></th></tr><tr><td style="text-align: right">Copula</td><td style="text-align: right">copula dependence params + 6 (H0 marginals) + 6 (H1 marginals) + 1 (mixing weight)</td><td style="text-align: right"><span>$k_{\text{cop}} + 13$</span></td></tr><tr><td style="text-align: right">Latent class</td><td style="text-align: right">2 classes <span>$\times$</span> 3 dims <span>$\times$</span> 2 (<span>$\mu, \sigma$</span>) + 1 (mixing weight)</td><td style="text-align: right">13</td></tr></table><p>Model weights are computed from BIC differences:</p><p class="math-container">\[w_m = \frac{\exp(-\tfrac{1}{2}\,\Delta\text{BIC}_m)}{\sum_j \exp(-\tfrac{1}{2}\,\Delta\text{BIC}_j)}, \quad \Delta\text{BIC}_m = \text{BIC}_m - \min_j \text{BIC}_j\]</p><h4 id="Averaged-Posterior"><a class="docs-heading-anchor" href="#Averaged-Posterior">Averaged Posterior</a><a id="Averaged-Posterior-1"></a><a class="docs-heading-anchor-permalink" href="#Averaged-Posterior" title="Permalink"></a></h4><p>The model-averaged posterior probability for each protein is a weighted combination:</p><p class="math-container">\[P_{\text{avg}}(H_1 \mid D_i) = w_{\text{copula}} \cdot P_{\text{copula}}(H_1 \mid D_i) + w_{\text{lc}} \cdot P_{\text{lc}}(H_1 \mid D_i)\]</p><p>The averaged Bayes factor is derived from the averaged posterior:</p><p class="math-container">\[BF_{\text{avg},i} = \frac{P_{\text{avg}} / (1 - P_{\text{avg}})}{\pi_1 / \pi_0}\]</p><p>where <span>$\pi_1 / \pi_0$</span> is the prior odds from the copula EM fit.</p><h2 id="Summary-Statistics"><a class="docs-heading-anchor" href="#Summary-Statistics">Summary Statistics</a><a id="Summary-Statistics-1"></a><a class="docs-heading-anchor-permalink" href="#Summary-Statistics" title="Permalink"></a></h2><p>For each protein, BayesInteractomics reports:</p><h3 id="Bayes-Factors-2"><a class="docs-heading-anchor" href="#Bayes-Factors-2">Bayes Factors</a><a class="docs-heading-anchor-permalink" href="#Bayes-Factors-2" title="Permalink"></a></h3><ul><li>Individual BFs from each model</li><li>Combined BF from copula mixture</li><li>Log BF for extremely large values</li></ul><h3 id="Posterior-Summaries-for-log2FC"><a class="docs-heading-anchor" href="#Posterior-Summaries-for-log2FC">Posterior Summaries for log2FC</a><a id="Posterior-Summaries-for-log2FC-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Summaries-for-log2FC" title="Permalink"></a></h3><ul><li><strong>Mean</strong>: <span>$\mathbb{E}[\log_2 FC | D]$</span></li><li><strong>Median</strong>: <span>$\text{median}(\log_2 FC | D)$</span></li><li><strong>SD</strong>: Standard deviation (uncertainty)</li><li><strong>Credible intervals</strong>: 95% highest density intervals</li><li><strong>Probability of direction (pd)</strong>: <span>$P(\log_2 FC &gt; 0 | D)$</span></li><li><strong>ROPE percentage</strong>: <span>$P(|\log_2 FC| &lt; \epsilon | D)$</span> where <span>$\epsilon$</span> is a practical equivalence threshold</li></ul><h3 id="Probability-of-Direction-(pd)"><a class="docs-heading-anchor" href="#Probability-of-Direction-(pd)">Probability of Direction (pd)</a><a id="Probability-of-Direction-(pd)-1"></a><a class="docs-heading-anchor-permalink" href="#Probability-of-Direction-(pd)" title="Permalink"></a></h3><p>The probability of direction (also called Maximum Probability of Effect) quantifies the certainty about the sign of an effect. For a posterior distribution of the parameter <span>$\theta$</span>:</p><p><strong>From posterior draws</strong> (<span>$S$</span> samples):</p><p class="math-container">\[\text{pd} = \max\!\left(\frac{1}{S}\sum_{s=1}^{S} \mathbb{1}[\theta^{(s)} &gt; 0],\;\; 1 - \frac{1}{S}\sum_{s=1}^{S} \mathbb{1}[\theta^{(s)} &gt; 0]\right)\]</p><p><strong>From an analytical posterior</strong> (e.g., Normal or mixture):</p><p class="math-container">\[\text{pd} = \max\!\left(\Phi(0),\; 1 - \Phi(0)\right)\]</p><p>where <span>$\Phi(0)$</span> is the CDF evaluated at zero.</p><p>The probability of direction is always in <span>$[0.5, 1.0]$</span>. A value of 0.5 indicates complete uncertainty about the direction, while 1.0 indicates certainty. The associated direction label is &quot;+&quot; if the effect is more likely positive, &quot;<span>$-$</span>&quot; if negative, or &quot;~&quot; if exactly 0.5.</p><p><strong>Conversion to p-values</strong> (for compatibility with frequentist frameworks):</p><table><tr><th style="text-align: right">Conversion</th><th style="text-align: right">Formula</th></tr><tr><td style="text-align: right">Two-sided p-value</td><td style="text-align: right"><span>$p = 2(1 - \text{pd})$</span></td></tr><tr><td style="text-align: right">One-sided p-value</td><td style="text-align: right"><span>$p = 1 - \text{pd}$</span></td></tr></table><h3 id="Bayesian-FDR-q-values"><a class="docs-heading-anchor" href="#Bayesian-FDR-q-values">Bayesian FDR q-values</a><a id="Bayesian-FDR-q-values-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-FDR-q-values" title="Permalink"></a></h3><p>To control the false discovery rate in a Bayesian framework, BayesInteractomics computes q-values from the posterior probabilities. The local false discovery rate for protein <span>$i$</span> is:</p><p class="math-container">\[\text{lfdr}_i = 1 - P(H_1 \mid D_i)\]</p><p>Proteins are sorted by descending posterior probability, and the q-value for the protein at rank <span>$i$</span> is the cumulative average of local false discovery rates up to that rank:</p><p class="math-container">\[q_i = \frac{1}{i} \sum_{j=1}^{i} \text{lfdr}_{(j)}\]</p><p>where <span>$(j)$</span> denotes the <span>$j$</span>-th protein in the sorted order. A protein with <span>$q_i &lt; \alpha$</span> means that among all proteins ranked at least as highly, the expected proportion of false discoveries is at most <span>$\alpha$</span>.</p><h3 id="Convergence-Diagnostics"><a class="docs-heading-anchor" href="#Convergence-Diagnostics">Convergence Diagnostics</a><a id="Convergence-Diagnostics-1"></a><a class="docs-heading-anchor-permalink" href="#Convergence-Diagnostics" title="Permalink"></a></h3><ul><li><strong>ESS (Effective Sample Size)</strong>: Measures quality of posterior samples (should be &gt; 400)</li><li><strong>Rhat</strong>: Gelman-Rubin convergence diagnostic (should be &lt; 1.01)</li></ul><h2 id="Multiple-Imputation"><a class="docs-heading-anchor" href="#Multiple-Imputation">Multiple Imputation</a><a id="Multiple-Imputation-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Imputation" title="Permalink"></a></h2><p>Mass spectrometry data frequently contain missing values (missing not at random or missing at random). BayesInteractomics supports <strong>multiple imputation</strong> to propagate missing-data uncertainty into all downstream inferences.</p><p>The procedure follows Rubin&#39;s (1987) combining rules:</p><ol><li><strong>Generate <span>$M$</span> imputed datasets</strong>: Each imputed dataset fills in missing values from a plausible imputation model.</li><li><strong>Fit the full model on each imputed dataset</strong>: The enrichment model (HBM) and regression model are run independently on each of the <span>$M$</span> datasets, yielding <span>$M$</span> posterior distributions per protein.</li><li><strong>Pool posteriors as an equal-weight mixture</strong>: For each protein parameter <span>$\theta$</span>, the pooled posterior is a mixture of the <span>$M$</span> individual posteriors:</li></ol><p class="math-container">\[q_{\text{pooled}}(\theta) = \frac{1}{M} \sum_{m=1}^{M} q_m(\theta)\]</p><p>where <span>$q_m(\theta)$</span> is the posterior from the <span>$m$</span>-th imputed dataset.</p><ol><li><strong>Compute statistics on the mixture</strong>: Bayes factors, log2FC summaries, credible intervals, and all other summary statistics are computed from the pooled mixture distribution. This naturally incorporates both within-imputation uncertainty (each <span>$q_m$</span> has its own spread) and between-imputation uncertainty (the <span>$q_m$</span> may have different locations).</li></ol><h2 id="Differential-Analysis"><a class="docs-heading-anchor" href="#Differential-Analysis">Differential Analysis</a><a id="Differential-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-Analysis" title="Permalink"></a></h2><p>BayesInteractomics provides a framework for comparing interaction profiles between two experimental conditions (e.g., wild-type vs. mutant, treated vs. untreated). For each protein present in both conditions, the analysis quantifies whether the interaction evidence differs.</p><h3 id="Differential-Bayes-Factor"><a class="docs-heading-anchor" href="#Differential-Bayes-Factor">Differential Bayes Factor</a><a id="Differential-Bayes-Factor-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-Bayes-Factor" title="Permalink"></a></h3><p>The differential Bayes factor measures relative evidence between condition A and condition B:</p><p class="math-container">\[\text{dBF}_i = \frac{BF_{i,A}}{BF_{i,B}}\]</p><p>computed in log-space as <span>$\log_{10}(\text{dBF}_i) = \log_{10}(BF_{i,A}) - \log_{10}(BF_{i,B})$</span>. A positive <span>$\log_{10}(\text{dBF})$</span> indicates stronger interaction evidence in condition A. Both the combined Bayes factor and per-evidence Bayes factors (enrichment, correlation, detection) are compared, allowing diagnosis of which evidence arm drives the differential signal.</p><h3 id="Effect-Size-and-Differential-Posterior"><a class="docs-heading-anchor" href="#Effect-Size-and-Differential-Posterior">Effect Size and Differential Posterior</a><a id="Effect-Size-and-Differential-Posterior-1"></a><a class="docs-heading-anchor-permalink" href="#Effect-Size-and-Differential-Posterior" title="Permalink"></a></h3><p>The effect size is the difference in mean log2 fold changes:</p><p class="math-container">\[\Delta\text{log2FC}_i = \overline{\text{log2FC}}_{i,A} - \overline{\text{log2FC}}_{i,B}\]</p><p>The differential posterior probability quantifies evidence for <em>any</em> difference (direction-agnostic):</p><p class="math-container">\[P(\text{diff} \mid D_i) = \frac{|\text{dBF}_i|}{1 + |\text{dBF}_i|}\]</p><p>Multiple testing is controlled by computing Bayesian FDR q-values on the differential posteriors (see <a href="#Bayesian-FDR-q-values">Bayesian FDR q-values</a> above).</p><h3 id="Interaction-Classification"><a class="docs-heading-anchor" href="#Interaction-Classification">Interaction Classification</a><a id="Interaction-Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Interaction-Classification" title="Permalink"></a></h3><p>Each protein is classified into one of five categories based on the differential evidence and configurable thresholds:</p><table><tr><th style="text-align: right">Class</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>GAINED</code></td><td style="text-align: right">Interaction is stronger or exclusively present in condition A</td></tr><tr><td style="text-align: right"><code>REDUCED</code></td><td style="text-align: right">Interaction is stronger or exclusively present in condition B</td></tr><tr><td style="text-align: right"><code>UNCHANGED</code></td><td style="text-align: right">No significant differential evidence</td></tr><tr><td style="text-align: right"><code>BOTH_NEGATIVE</code></td><td style="text-align: right">Neither condition shows interaction, but differential q-value is significant</td></tr><tr><td style="text-align: right"><code>CONDITION_SPECIFIC</code></td><td style="text-align: right">Protein detected in only one condition (appended as <code>CONDITION_A_SPECIFIC</code> or <code>CONDITION_B_SPECIFIC</code>)</td></tr></table><p>Three classification methods are available:</p><ul><li><strong>Posterior</strong>: uses per-condition posterior probability thresholds and <span>$\Delta\text{log2FC}$</span></li><li><strong>dBF</strong>: uses <span>$|\log_{10}(\text{dBF})|$</span> exceeding a threshold</li><li><strong>Combined</strong>: requires both posterior and dBF criteria to hold simultaneously</li></ul><h2 id="Computational-Implementation"><a class="docs-heading-anchor" href="#Computational-Implementation">Computational Implementation</a><a id="Computational-Implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Computational-Implementation" title="Permalink"></a></h2><h3 id="Parallelization"><a class="docs-heading-anchor" href="#Parallelization">Parallelization</a><a id="Parallelization-1"></a><a class="docs-heading-anchor-permalink" href="#Parallelization" title="Permalink"></a></h3><p>BayesInteractomics exploits multi-core parallelism:</p><ul><li>Proteins are analyzed independently in parallel using Julia&#39;s multi-threading</li><li>Each thread writes results to a separate cache file to avoid contention</li><li>Results are merged after all proteins complete</li></ul><h3 id="Variational-Inference"><a class="docs-heading-anchor" href="#Variational-Inference">Variational Inference</a><a id="Variational-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Variational-Inference" title="Permalink"></a></h3><p>RxInfer.jl uses <strong>variational message passing</strong> for fast Bayesian inference:</p><ul><li>Factorized approximation: <span>$q(\theta) = \prod_i q_i(\theta_i)$</span></li><li>Iterative message passing updates until convergence</li><li>Automatically handles missing data through marginalization</li></ul><h3 id="Numerical-Stability"><a class="docs-heading-anchor" href="#Numerical-Stability">Numerical Stability</a><a id="Numerical-Stability-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-Stability" title="Permalink"></a></h3><ul><li>Log-space computation for extreme Bayes factors</li><li>Regularization of variance estimates (lower bound <span>$\epsilon = 10^{-6}$</span>)</li><li>Robust initialization for EM algorithm</li><li>Convergence checks with maximum iteration limits</li></ul><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><h3 id="Bayesian-Inference"><a class="docs-heading-anchor" href="#Bayesian-Inference">Bayesian Inference</a><a id="Bayesian-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Inference" title="Permalink"></a></h3><ul><li>Gelman, A., et al. (2013). <em>Bayesian Data Analysis</em>, 3rd ed. Chapman &amp; Hall/CRC.</li><li>Kruschke, J. K. (2014). <em>Doing Bayesian Data Analysis</em>, 2nd ed. Academic Press.</li></ul><h3 id="Bayes-Factors-3"><a class="docs-heading-anchor" href="#Bayes-Factors-3">Bayes Factors</a><a class="docs-heading-anchor-permalink" href="#Bayes-Factors-3" title="Permalink"></a></h3><ul><li>Kass, R. E., &amp; Raftery, A. E. (1995). Bayes factors. <em>Journal of the American Statistical Association</em>, 90(430), 773-795.</li><li>Rouder, J. N., et al. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. <em>Psychonomic Bulletin &amp; Review</em>, 16(2), 225-237.</li></ul><h3 id="Hierarchical-Models"><a class="docs-heading-anchor" href="#Hierarchical-Models">Hierarchical Models</a><a id="Hierarchical-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Hierarchical-Models" title="Permalink"></a></h3><ul><li>Gelman, A., &amp; Hill, J. (2006). <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge University Press.</li></ul><h3 id="Robust-Regression"><a class="docs-heading-anchor" href="#Robust-Regression">Robust Regression</a><a id="Robust-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Robust-Regression" title="Permalink"></a></h3><ul><li>Lange, K. L., Little, R. J. A., &amp; Taylor, J. M. G. (1989). Robust statistical modeling using the t distribution. <em>Journal of the American Statistical Association</em>, 84(408), 881-896.</li></ul><h3 id="Model-Comparison"><a class="docs-heading-anchor" href="#Model-Comparison">Model Comparison</a><a id="Model-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Comparison" title="Permalink"></a></h3><ul><li>Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. <em>Journal of Machine Learning Research</em>, 11, 3571-3594.</li><li>Gelman, A., Hwang, J., &amp; Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. <em>Statistics and Computing</em>, 24(6), 997-1016.</li><li>Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. <em>Statistics and Computing</em>, 27(5), 1413-1432.</li></ul><h3 id="Copula-Theory-2"><a class="docs-heading-anchor" href="#Copula-Theory-2">Copula Theory</a><a class="docs-heading-anchor-permalink" href="#Copula-Theory-2" title="Permalink"></a></h3><ul><li>Nelsen, R. B. (2006). <em>An Introduction to Copulas</em>, 2nd ed. Springer.</li><li>Joe, H. (2014). <em>Dependence Modeling with Copulas</em>. Chapman &amp; Hall/CRC.</li></ul><h3 id="Mixture-Models-and-BMA"><a class="docs-heading-anchor" href="#Mixture-Models-and-BMA">Mixture Models and BMA</a><a id="Mixture-Models-and-BMA-1"></a><a class="docs-heading-anchor-permalink" href="#Mixture-Models-and-BMA" title="Permalink"></a></h3><ul><li>McLachlan, G. J., &amp; Peel, D. (2000). <em>Finite Mixture Models</em>. Wiley.</li><li>Hoeting, J. A., Madigan, D., Raftery, A. E., &amp; Volinsky, C. T. (1999). Bayesian model averaging: a tutorial. <em>Statistical Science</em>, 14(4), 382-417.</li></ul><h3 id="Summary-Statistics-2"><a class="docs-heading-anchor" href="#Summary-Statistics-2">Summary Statistics</a><a class="docs-heading-anchor-permalink" href="#Summary-Statistics-2" title="Permalink"></a></h3><ul><li>Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., &amp; Lüdecke, D. (2019). Indices of effect existence and significance in the Bayesian framework. <em>Frontiers in Psychology</em>, 10, 2767.</li><li>Efron, B., Tibshirani, R., Storey, J. D., &amp; Tusher, V. (2001). Empirical Bayes analysis of a microarray experiment. <em>Journal of the American Statistical Association</em>, 96(456), 1151-1160.</li></ul><h3 id="Multiple-Imputation-2"><a class="docs-heading-anchor" href="#Multiple-Imputation-2">Multiple Imputation</a><a class="docs-heading-anchor-permalink" href="#Multiple-Imputation-2" title="Permalink"></a></h3><ul><li>Rubin, D. B. (1987). <em>Multiple Imputation for Nonresponse in Surveys</em>. Wiley.</li></ul><h3 id="Variational-Inference-2"><a class="docs-heading-anchor" href="#Variational-Inference-2">Variational Inference</a><a class="docs-heading-anchor-permalink" href="#Variational-Inference-2" title="Permalink"></a></h3><ul><li>Blei, D. M., et al. (2017). Variational inference: A review for statisticians. <em>Journal of the American Statistical Association</em>, 112(518), 859-877.</li><li>Bagaev, D., &amp; de Vries, B. (2023). RxInfer: A Julia package for reactive message-passing-based Bayesian inference. <em>Journal of Open Source Software</em>, 8(84), 5161.</li></ul><h3 id="Proteomics-Applications"><a class="docs-heading-anchor" href="#Proteomics-Applications">Proteomics Applications</a><a id="Proteomics-Applications-1"></a><a class="docs-heading-anchor-permalink" href="#Proteomics-Applications" title="Permalink"></a></h3><ul><li>Choi, H., et al. (2011). SAINT: Probabilistic scoring of affinity purification-mass spectrometry data. <em>Nature Methods</em>, 8(1), 70-73.</li><li>Mellacheruvu, D., et al. (2013). The CRAPome: A contaminant repository for affinity purification-mass spectrometry data. <em>Nature Methods</em>, 10(8), 730-736.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../api/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Saturday 28 February 2026 08:35">Saturday 28 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
